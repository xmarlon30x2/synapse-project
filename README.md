Synapse AI

https://img.shields.io/badge/License-MIT-yellow.svg
https://img.shields.io/badge/python-3.13+-blue.svg
https://img.shields.io/badge/code%20style-black-000000.svg

Synapse AI es un framework modular y flexible para construir agentes conversacionales potenciados por modelos de lenguaje (LLMs) y herramientas externas, utilizando el Model Context Protocol (MCP). DiseÃ±ado para ser simple de usar pero altamente extensible, te permite conectar tu agente a cualquier API compatible con OpenAI (OpenRouter, OpenAI, proveedores locales, etc.) y a servidores MCP que exponen herramientas como sistema de archivos, bÃºsqueda web, bases de datos y mÃ¡s.

---

âœ¨ CaracterÃ­sticas

Â· Arquitectura limpia y desacoplada: SeparaciÃ³n clara entre agente, memoria, modelo y contexto (gestiÃ³n de herramientas).
Â· Soporte nativo para herramientas MCP: Conecta mÃºltiples servidores MCP y el agente podrÃ¡ invocar sus herramientas automÃ¡ticamente.
Â· Streaming en tiempo real: Observa la generaciÃ³n de la respuesta token por token, incluyendo las llamadas a herramientas y sus resultados.
Â· Memoria persistente: La conversaciÃ³n se guarda automÃ¡ticamente en un archivo JSON y se restaura al reiniciar.
Â· CLI interactiva con comandos integrados: Usa /help, /clear, /exit, /send para gestionar la conversaciÃ³n fÃ¡cilmente.
Â· Totalmente asÃ­ncrono: Construido con asyncio para un rendimiento Ã³ptimo en operaciones de E/S.
Â· FÃ¡cilmente extensible: Puedes crear tus propias implementaciones de memoria, modelo o contexto.

---

ğŸ“¦ InstalaciÃ³n

OpciÃ³n 1: InstalaciÃ³n desde PyPI (cuando estÃ© publicado)

```bash
pip install synapse-ai
```

OpciÃ³n 2: InstalaciÃ³n desde el repositorio

```bash
git clone https://github.com/xmarlon30x2/synapse-ai.git
cd synapse-ai
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -e .
```

Requisitos

Â· Python 3.13 o superior.
Â· Una clave de API para el proveedor de modelos (OpenRouter, OpenAI, etc.)

---

âš™ï¸ ConfiguraciÃ³n

Crea un archivo .env en el directorio de trabajo (o define la variable de entorno SYNAPSE_APIKEY) con tu clave de API:

```env
SYNAPSE_APIKEY=sk-or-v1-...
```

AdemÃ¡s, necesitas dos archivos JSON (puedes cambiar sus rutas con argumentos de lÃ­nea de comandos):

1. Contexto de herramientas (synapse-context.json)

Define los servidores MCP que el agente puede utilizar. Ejemplo con el servidor de sistema de archivos:

```json
{
  "servers": [
    {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/ruta/a/tu/directorio"],
      "env": {}
    }
  ]
}
```

Cada servidor debe ser un ejecutable que implemente el protocolo MCP sobre stdio.

2. Archivo de memoria (synapse-memory.json)

Se crea automÃ¡ticamente y almacena el historial de la conversaciÃ³n. No es necesario editarlo manualmente.

---

ğŸš€ Uso

Una vez instalado, ejecuta la CLI con:

```bash
synapse-cli [opciones]
```

Opciones disponibles

OpciÃ³n DescripciÃ³n
--context-filename Ruta al JSON de configuraciÃ³n del contexto (por defecto ./synapse-context.json)
--memory-filename Ruta al archivo de memoria (por defecto ./synapse-memory.json)
--api-key Clave de API (sobrescribe la variable de entorno SYNAPSE_APIKEY)
--base-url URL base del proveedor de IA (por defecto https://openrouter.ai/api/v1)
--model Nombre del modelo (por defecto openrouter/aurora-alpha)

Ejemplo

```bash
synapse-cli --model openai/gpt-4o --base-url https://api.openai.com/v1
```

Al iniciar, verÃ¡s el prompt interactivo:

```
Welcome to Synapse CLI!

Write /help to show help
Write anything to run the agent

user>
```

Comandos integrados

Comando DescripciÃ³n
/exit Sale del programa.
/help Muestra esta ayuda.
/clear Borra todo el historial de la conversaciÃ³n.
/send <msg> EnvÃ­a un mensaje al agente sin ejecutarlo (solo lo almacena en memoria).

Todo lo que no empiece con / se envÃ­a al agente, que lo procesa y transmite la respuesta en tiempo real.

---

ğŸ§  Ejemplo de interacciÃ³n

```
user> Â¿QuÃ© archivos hay en el directorio actual?
(call:call_abc123) list_directory> { "path": "." }
(tool:call_abc123) ["README.md", "src", "pyproject.toml"]
En el directorio actual encuentro los archivos README.md, la carpeta src y pyproject.toml.
```

Durante la respuesta, los tokens de texto aparecen en lÃ­nea, y las llamadas a herramientas se muestran con su ID y resultados.

---

ğŸ—ï¸ Arquitectura del proyecto

Synapse AI se compone de cuatro mÃ³dulos principales:

Â· Agent: Orquesta el ciclo de conversaciÃ³n. Decide cuÃ¡ndo invocar al modelo y cuÃ¡ndo ejecutar herramientas.
Â· Memory: Almacena todos los mensajes (usuario, asistente, herramientas). La implementaciÃ³n por defecto usa un archivo JSON.
Â· Model: Encapsula la comunicaciÃ³n con la API del LLM. Maneja streaming y conversiÃ³n de formatos.
Â· Context: Gestiona las conexiones a servidores MCP. Provee las definiciones de herramientas y ejecuta las llamadas.

Flujo tÃ­pico:

1. La entrada del usuario se guarda en memoria.
2. El agente invoca al modelo en modo streaming.
3. Si el modelo genera llamadas a herramientas, el contexto las ejecuta y los resultados se aÃ±aden como mensajes de herramienta.
4. El agente vuelve a llamar al modelo con esos resultados para obtener la respuesta final.
5. La respuesta final se transmite al usuario.

---

ğŸ› ï¸ Desarrollo

Si deseas contribuir o modificar el cÃ³digo:

1. Clona el repositorio e instala en modo editable (como se indicÃ³ arriba).
2. AsegÃºrate de tener las herramientas de desarrollo:
   ```bash
   pip install black ruff mypy
   ```
3. El cÃ³digo sigue las configuraciones de pyproject.toml:
   Â· Formato: black src/
   Â· Linting: ruff check src/
   Â· Tipado: mypy src/

Ejecuta todas las comprobaciones antes de enviar un Pull Request.

---

ğŸ“„ Licencia

Distribuido bajo la licencia MIT. Ver LICENSE para mÃ¡s informaciÃ³n.

---

ğŸ”— Enlaces

Â· Repositorio en GitHub
Â· DocumentaciÃ³n
Â· Issues

---

Â¿Preguntas o sugerencias? No dudes en abrir un issue. Â¡Las contribuciones son bienvenidas!